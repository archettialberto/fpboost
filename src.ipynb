{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPBoost: Fully Parametric Gradient Boosting for Survival Analysis\n",
    "\n",
    "Source code of the paper \"FPBoost: Fully Parametric Gradient Boosting for Survival Analysis\" for AAAI 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearndf.pipeline import PipelineDF\n",
    "from sklearndf.transformation import (\n",
    "    ColumnTransformerDF,\n",
    "    OneHotEncoderDF,\n",
    "    SimpleImputerDF,\n",
    "    StandardScalerDF,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest as RSF, GradientBoostingSurvivalAnalysis\n",
    "\n",
    "try:\n",
    "    from auton_survival.models.dsm import DeepSurvivalMachines as DSM\n",
    "except ImportError:\n",
    "    os.system(\"git clone https://github.com/autonlab/auton-survival.git\")\n",
    "    os.system(\"mv auton-survival/auton_survival .\")\n",
    "    os.system(\"rm -r auton-survival\")\n",
    "    from auton_survival.models.dsm import DeepSurvivalMachines as DSM\n",
    "\n",
    "import numba\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtuples as tt\n",
    "from pycox.models import DeepHitSingle, CoxPH\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import optuna\n",
    "\n",
    "from utils.data_loader import load_dataframe\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "This section defines the data loading and preprocessing functions alongsiide the cross-validation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 10  # Number of folds for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_transformer():\n",
    "    \"\"\"Returns the preprocessing sklearn transformer.\"\"\"\n",
    "    sel_fac = make_column_selector(pattern=\"^fac\\\\_\")\n",
    "    enc_fac = PipelineDF(\n",
    "        steps=[(\"ohe\", OneHotEncoderDF(sparse_output=False, handle_unknown=\"ignore\"))]\n",
    "    )\n",
    "    sel_num = make_column_selector(pattern=\"^num\\\\_\")\n",
    "    enc_num = PipelineDF(\n",
    "        steps=[\n",
    "            (\"impute\", SimpleImputerDF(strategy=\"median\")),\n",
    "            (\"scale\", StandardScalerDF()),\n",
    "        ]\n",
    "    )\n",
    "    tr = ColumnTransformerDF(transformers=[(\"ohe\", enc_fac, sel_fac), (\"s\", enc_num, sel_num)])\n",
    "    return tr\n",
    "\n",
    "\n",
    "def get_k_fold_splits(df):\n",
    "    \"\"\"Returns a generator of k-fold splits.\"\"\"\n",
    "    events = df[\"event\"].values.astype(bool)\n",
    "    times = df[\"time\"].values\n",
    "    times = times / times.max()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = list(skf.split(df, events))\n",
    "    splits = [s[1] for s in splits]\n",
    "\n",
    "    for i in range(FOLDS):\n",
    "        val_idx = splits[i]\n",
    "        test_idx = splits[(i + 1) % FOLDS]\n",
    "        train_idx = [j for j in range(len(df)) if j not in val_idx and j not in test_idx]\n",
    "\n",
    "        df_train = df.iloc[train_idx]\n",
    "        df_val = df.iloc[val_idx]\n",
    "        df_test = df.iloc[test_idx]\n",
    "\n",
    "        tr = get_preprocess_transformer()\n",
    "        X_train = tr.fit_transform(df_train).to_numpy().astype(np.float32)\n",
    "        X_val = tr.transform(df_val).to_numpy().astype(np.float32)\n",
    "        X_test = tr.transform(df_test).to_numpy().astype(np.float32)\n",
    "\n",
    "        e_train = events[train_idx]\n",
    "        e_val = events[val_idx]\n",
    "        e_test = events[test_idx]\n",
    "\n",
    "        t_train = times[train_idx]\n",
    "        t_val = times[val_idx]\n",
    "        t_test = times[test_idx]\n",
    "\n",
    "        max_time = t_train.max()\n",
    "        min_time = t_train.min()\n",
    "        X_val = X_val[(min_time < t_val) & (t_val < max_time)]\n",
    "        e_val = e_val[(min_time < t_val) & (t_val < max_time)]\n",
    "        t_val = t_val[(min_time < t_val) & (t_val < max_time)]\n",
    "        X_test = X_test[(min_time < t_test) & (t_test < max_time)]\n",
    "        e_test = e_test[(min_time < t_test) & (t_test < max_time)]\n",
    "        t_test = t_test[(min_time < t_test) & (t_test < max_time)]\n",
    "\n",
    "        sksurv_type = [(\"event\", bool), (\"time\", float)]\n",
    "        y_train = np.array([(e, t) for e, t in zip(e_train, t_train)], dtype=sksurv_type)\n",
    "        y_val = np.array([(e, t) for e, t in zip(e_val, t_val)], dtype=sksurv_type)\n",
    "        y_test = np.array([(e, t) for e, t in zip(e_test, t_test)], dtype=sksurv_type)\n",
    "\n",
    "        yield (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "This section defines the FPBoost model and the base learners, all implementing the `SurvModel` abstract class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def concordance_index_td(\n",
    "    events: np.ndarray, times: np.ndarray, risks: np.ndarray, percentile: float = 1.0\n",
    ") -> float:\n",
    "    \"\"\"Computes the concordance index for time-dependent data.\"\"\"\n",
    "    threshold_time = np.percentile(times, percentile * 100)\n",
    "    concordant_pairs, comparable_pairs = 0, 0\n",
    "    for i, ti in enumerate(times):\n",
    "        for j, tj in enumerate(times):\n",
    "            if events[i] == 1 and ti < tj and ti < threshold_time:\n",
    "                comparable_pairs += 1\n",
    "                if risks[i] > risks[j]:\n",
    "                    concordant_pairs += 1\n",
    "    return concordant_pairs / comparable_pairs if comparable_pairs > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvModel(ABC):\n",
    "    \"\"\"Base class for survival models.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Fits the model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train: Training data of shape (n_samples, n_features).\n",
    "            y_train: Training labels of shape (n_samples,) with dtype=[(\"event\", bool), (\"time\", float)].\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        \"\"\"Predicts the survival function for the given times.\n",
    "\n",
    "        Args:\n",
    "            X_test: Test data of shape (n_samples, n_features).\n",
    "            times: Times at which to predict the survival function of shape (n_times,).\n",
    "\n",
    "        Returns:\n",
    "            Survival function of shape (n_samples, n_times).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, X_test, y_test, y_train) -> dict[str, float]:\n",
    "        \"\"\"Evaluates the model on the test data.\n",
    "\n",
    "        Args:\n",
    "            X_test: Test data of shape (n_samples, n_features).\n",
    "            y_test: Test labels of shape (n_samples,) with dtype=[(\"event\", bool), (\"time\", float)].\n",
    "            y_train: Training labels of shape (n_samples,) with dtype=[(\"event\", bool), (\"time\", float)].\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of survival metrics.\n",
    "        \"\"\"\n",
    "        min_time, max_time = y_test[\"time\"].min(), y_test[\"time\"].max()\n",
    "        tolerance = 0.1 * (max_time - min_time)\n",
    "        times = np.linspace(min_time + tolerance, max_time - tolerance, 100)\n",
    "        survs = self.predict(X_test, times)\n",
    "        mean_times = survs.sum(axis=1)\n",
    "        c25 = concordance_index_td(y_test[\"event\"], y_test[\"time\"], -mean_times, 0.25)\n",
    "        c50 = concordance_index_td(y_test[\"event\"], y_test[\"time\"], -mean_times, 0.50)\n",
    "        c75 = concordance_index_td(y_test[\"event\"], y_test[\"time\"], -mean_times, 0.75)\n",
    "        cid = concordance_index_td(y_test[\"event\"], y_test[\"time\"], -mean_times)\n",
    "        try:\n",
    "            ibs = integrated_brier_score(y_train, y_test, survs, times)\n",
    "        except ValueError as e:\n",
    "            ibs = 0.25\n",
    "        return {\n",
    "            \"cid\": cid,\n",
    "            \"ibs\": ibs,\n",
    "            \"c25\": c25,\n",
    "            \"c50\": c50,\n",
    "            \"c75\": c75,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cox(SurvModel):\n",
    "    \"\"\"Cox proportional hazards model (Cox, 1972).\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = CoxPHSurvivalAnalysis(alpha=0.01)\n",
    "        self.failed_opt = False\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        try:\n",
    "            self.model.fit(X_train, y_train)\n",
    "        except ValueError as e:\n",
    "            self.failed_opt = True\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        if self.failed_opt:\n",
    "            return np.ones((X_test.shape[0], len(times))) * 0.5\n",
    "        return np.array([S(times) for S in self.model.predict_survival_function(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSurvivalForest(SurvModel):\n",
    "    \"\"\"Random survival forest model (Ishwaran et al., 2008).\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = RSF(n_jobs=-1)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        return np.array([S(times) for S in self.model.predict_survival_function(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurv(SurvModel):\n",
    "    \"\"\"DeepSurv model (Katzman et al., 2018).\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.net = None\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        net = tt.practical.MLPVanilla(\n",
    "            X_train.shape[1],\n",
    "            [i * X_train.shape[1] for i in [3, 5, 3]],  # hidden layers as in Katzman et al., 2018\n",
    "            1,  # outputs\n",
    "            False,  # batch norm\n",
    "            0.6,  # dropout, as in Katzman et al., 2018\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, stratify=y_train[\"event\"]\n",
    "        )\n",
    "        _y_val = (y_val[\"time\"].copy(), y_val[\"event\"].copy())\n",
    "        val_data = (X_val, _y_val)\n",
    "        callbacks = [tt.callbacks.EarlyStopping()]\n",
    "        self.model = CoxPH(net, tt.optim.Adam)\n",
    "        _y_train = (y_train[\"time\"].copy(), y_train[\"event\"].copy())\n",
    "        self.model.fit(\n",
    "            X_train, _y_train, 256, 256, val_data=val_data, callbacks=callbacks, verbose=False\n",
    "        )\n",
    "        self.model.compute_baseline_hazards()\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        preds = self.model.predict_surv_df(X_test)\n",
    "        unique_times = preds.index.to_numpy()\n",
    "        survs = np.array([preds.iloc[:, i].values for i in range(preds.shape[1])])\n",
    "        surv = np.zeros((X_test.shape[0], len(times)))\n",
    "        for i, t in enumerate(times):\n",
    "            idx = np.abs(unique_times - t).argmin()\n",
    "            surv[:, i] = survs[:, idx]\n",
    "        return surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepHit(SurvModel):\n",
    "    \"\"\"DeepHit model (Lee et al., 2018).\"\"\"\n",
    "\n",
    "    def __init__(self, num_durations: int = 5) -> None:\n",
    "        self.net = None\n",
    "        self.model = None\n",
    "        self.labtrans = None\n",
    "        self.num_durations = num_durations\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, stratify=y_train[\"event\"]\n",
    "        )\n",
    "        self.labtrans = DeepHitSingle.label_transform(self.num_durations)\n",
    "        _y_train = self.labtrans.fit_transform(y_train[\"time\"], y_train[\"event\"])\n",
    "        _y_val = self.labtrans.transform(y_val[\"time\"], y_val[\"event\"])\n",
    "        val_data = (X_val, _y_val)\n",
    "        net = tt.practical.MLPVanilla(\n",
    "            X_train.shape[1],\n",
    "            [i * X_train.shape[1] for i in [3, 5, 3]],\n",
    "            self.num_durations,  # outputs\n",
    "            False,  # batch norm\n",
    "            0.6,  # dropout\n",
    "        )\n",
    "        callbacks = [tt.callbacks.EarlyStopping()]\n",
    "        self.model = DeepHitSingle(net, tt.optim.Adam, alpha=0.5, duration_index=self.labtrans.cuts)\n",
    "        self.model.fit(\n",
    "            X_train, _y_train, 256, 256, val_data=val_data, callbacks=callbacks, verbose=False\n",
    "        )\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        preds = self.model.predict_surv_df(X_test)\n",
    "        unique_times = preds.index.to_numpy()\n",
    "        survs = np.array([preds.iloc[:, i].values for i in range(preds.shape[1])])\n",
    "        surv = np.zeros((X_test.shape[0], len(times)))\n",
    "        for i, t in enumerate(times):\n",
    "            idx = np.abs(unique_times - t).argmin()\n",
    "            surv[:, i] = survs[:, idx]\n",
    "        return surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurvivalMachines(SurvModel):\n",
    "    \"\"\"Deep survival machines model (Nagpal et al., 2021).\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = DSM(layers=[i * X_train.shape[1] for i in [3, 5, 3]])\n",
    "        self.model.fit(X_train, y_train[\"time\"], y_train[\"event\"])\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        r = np.concatenate(\n",
    "            [\n",
    "                self.model.predict_risk(X_test.astype(np.float64), t.astype(np.float64))\n",
    "                for t in times\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        survs = np.exp(-r)\n",
    "        return survs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoxBoost(SurvModel):\n",
    "    \"\"\"CoxBoost model (Ridgeway, 1999).\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = GradientBoostingSurvivalAnalysis()\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        return np.array([S(times) for S in self.model.predict_survival_function(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPBoost(SurvModel):\n",
    "    \"\"\"FPBoost model for AAAI submission.\n",
    "\n",
    "    Args:\n",
    "        weibull_heads: Number of Weibull heads.\n",
    "        loglogistic_heads: Number of log-logistic heads.\n",
    "        n_estimators: Number of base learners per estimated parameter.\n",
    "        max_depth: Maximum depth of the base learners.\n",
    "        learning_rate: Learning rate for the boosting algorithm.\n",
    "        alpha: ElasticNet regularization strength.\n",
    "        l1_ratio: Ratio between L1 and L2 regularization.\n",
    "        uniform_heads: Whether to use uniform weights for the heads.\n",
    "        heads_activation: Activation function for the heads. Can be \"relu\" or \"softmax\".\n",
    "        patience: Patience for early stopping.\n",
    "        verbose: Whether to print progress.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weibull_heads: int,\n",
    "        loglogistic_heads: int,\n",
    "        n_estimators: int,\n",
    "        max_depth: int,\n",
    "        learning_rate: float,\n",
    "        alpha: float,\n",
    "        l1_ratio: float,\n",
    "        uniform_heads: bool,\n",
    "        heads_activation: str,\n",
    "        patience: Optional[int],\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.weibull_heads = weibull_heads\n",
    "        self.loglogistic_heads = loglogistic_heads\n",
    "        self.heads = weibull_heads + loglogistic_heads\n",
    "        if self.heads == 0:\n",
    "            self.weibull_heads = 1\n",
    "            self.heads = 1\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.uniform_heads = uniform_heads\n",
    "        self.heads_activation = heads_activation\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Random initialization of the parameters\n",
    "        self.init_eta = np.random.rand(self.heads) + 0.5\n",
    "        self.eta_heads = [[] for _ in range(self.heads)]\n",
    "        self.init_k = np.random.rand(self.heads) * 2\n",
    "        self.k_heads = [[] for _ in range(self.heads)]\n",
    "        self.init_w = np.random.rand(self.heads)\n",
    "        self.w_heads = [[] for _ in range(self.heads)]\n",
    "\n",
    "        heads_activation_fns = {\n",
    "            \"relu\": lambda w: F.relu(w),\n",
    "            \"softmax\": lambda w: F.softmax(w, dim=1),\n",
    "        }\n",
    "        if heads_activation not in heads_activation_fns:\n",
    "            raise ValueError(f\"Heads activation function not in {heads_activation_fns.keys()}\")\n",
    "        self.heads_activation_fn = heads_activation_fns[heads_activation]\n",
    "\n",
    "    def _predict_etas(self, X: np.array) -> np.array:\n",
    "        output = np.zeros((len(X), self.heads)) + self.init_eta.reshape((1, -1))\n",
    "        for i, regs in enumerate(self.eta_heads):\n",
    "            if len(regs) == 0:\n",
    "                continue\n",
    "            preds = np.concatenate([reg.predict(X).reshape((-1, 1)) for reg in regs], axis=1)\n",
    "            output[:, i] += self.learning_rate * np.sum(preds, axis=1)\n",
    "        return output\n",
    "\n",
    "    def _predict_ks(self, X: np.array) -> np.array:\n",
    "        output = np.ones((len(X), self.heads)) * self.init_k.reshape((1, -1))\n",
    "        for i, regs in enumerate(self.k_heads):\n",
    "            if len(regs) == 0:\n",
    "                continue\n",
    "            preds = np.concatenate([reg.predict(X).reshape((-1, 1)) for reg in regs], axis=1)\n",
    "            output[:, i] += self.learning_rate * np.sum(preds, axis=1)\n",
    "        return output\n",
    "\n",
    "    def _predict_ws(self, X: np.array) -> np.array:\n",
    "        if self.uniform_heads:\n",
    "            return np.ones((len(X), self.heads)) / self.heads\n",
    "        output = np.ones((len(X), self.heads)) * self.init_w.reshape((1, -1))\n",
    "        for i, regs in enumerate(self.w_heads):\n",
    "            if len(regs) == 0:\n",
    "                continue\n",
    "            preds = np.concatenate([reg.predict(X).reshape((-1, 1)) for reg in regs], axis=1)\n",
    "            output[:, i] += self.learning_rate * np.sum(preds, axis=1)\n",
    "        return output\n",
    "\n",
    "    def _predict_params(self, X: np.array) -> np.array:\n",
    "        etas = self._predict_etas(X).reshape((-1, self.heads, 1))\n",
    "        ks = self._predict_ks(X).reshape((-1, self.heads, 1))\n",
    "        ws = self._predict_ws(X).reshape((-1, self.heads, 1))\n",
    "        return np.concatenate([etas, ks, ws], -1)\n",
    "\n",
    "    def _weibull_hazard(self, eta, k, times):\n",
    "        return k * eta * times ** (k - 1)\n",
    "\n",
    "    def _weibull_cum_hazard(self, eta, k, times):\n",
    "        return eta * times**k\n",
    "\n",
    "    def _loglogistic_hazard(self, eta, k, times):\n",
    "        return eta * k * times ** (k - 1) / (1 + eta * times**k)\n",
    "\n",
    "    def _loglogistic_cum_hazard(self, eta, k, times):\n",
    "        if torch.is_tensor(times):\n",
    "            return torch.log1p(eta * times**k)\n",
    "        return np.log1p(eta * times**k)\n",
    "\n",
    "    def _get_neg_grads(self, params: np.array, events: Tensor, times: Tensor) -> np.array:\n",
    "        params_torch = Variable(torch.tensor(params).float(), requires_grad=True)\n",
    "\n",
    "        etas = F.relu(params_torch[:, :, 0])\n",
    "        ks = F.relu(params_torch[:, :, 1])\n",
    "        ws = self.heads_activation_fn(params_torch[:, :, 2])\n",
    "\n",
    "        hazard = torch.zeros(len(times))\n",
    "        cum_hazard = torch.zeros(len(times))\n",
    "\n",
    "        if self.weibull_heads > 0:\n",
    "            weibull_hazard = self._weibull_hazard(\n",
    "                etas[:, : self.weibull_heads], ks[:, : self.weibull_heads], times\n",
    "            )\n",
    "            weibull_cum_hazard = self._weibull_cum_hazard(\n",
    "                etas[:, : self.weibull_heads], ks[:, : self.weibull_heads], times\n",
    "            )\n",
    "            hazard += (weibull_hazard * ws[:, : self.weibull_heads]).sum(dim=1)\n",
    "            cum_hazard += (weibull_cum_hazard * ws[:, : self.weibull_heads]).sum(dim=1)\n",
    "\n",
    "        if self.loglogistic_heads > 0:\n",
    "            loglogistic_hazard = self._loglogistic_hazard(\n",
    "                etas[:, self.weibull_heads :], ks[:, self.weibull_heads :], times\n",
    "            )\n",
    "            loglogistic_cum_hazard = self._loglogistic_cum_hazard(\n",
    "                etas[:, self.weibull_heads :], ks[:, self.weibull_heads :], times\n",
    "            )\n",
    "            hazard += (loglogistic_hazard * ws[:, self.weibull_heads :]).sum(dim=1)\n",
    "            cum_hazard += (loglogistic_cum_hazard * ws[:, self.weibull_heads :]).sum(dim=1)\n",
    "\n",
    "        log_likelihood = (events * torch.log(hazard) - cum_hazard).mean()\n",
    "        l1_reg = torch.abs(params_torch).mean()\n",
    "        l2_reg = (params_torch**2).mean()\n",
    "        elastic_net_reg = self.l1_ratio * l1_reg + (1 - self.l1_ratio) * l2_reg\n",
    "        loss = -log_likelihood + self.alpha * elastic_net_reg\n",
    "\n",
    "        loss.backward()\n",
    "        grad = params_torch.grad.numpy()\n",
    "        grad[np.isnan(grad)] = 0.0\n",
    "        return -(grad / np.abs(grad).max())\n",
    "\n",
    "    def _fit_base_learner(self, X: np.array, y: np.array):\n",
    "        reg = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        reg.fit(X, y)\n",
    "        return reg\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array) -> None:\n",
    "        if self.verbose:\n",
    "            print(f\"Fitting a Survival Boosting model with {self.heads} heads...\")\n",
    "\n",
    "        patience_counter, best_num_base_learners, best_cid = 0, 0, 0.0\n",
    "        if self.patience is not None:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                test_size=0.2,\n",
    "                stratify=y_train[\"event\"],\n",
    "            )\n",
    "\n",
    "        events = torch.tensor(y_train[\"event\"].copy()).float().reshape((-1,))\n",
    "        times = torch.tensor(y_train[\"time\"].copy()).float().reshape((-1, 1))\n",
    "        timeline = np.linspace(np.min(y_train[\"time\"]), np.max(y_train[\"time\"]), 100)\n",
    "\n",
    "        for j in range(self.n_estimators):\n",
    "            params = self._predict_params(X_train)\n",
    "\n",
    "            neg_grads = self._get_neg_grads(params, events, times)\n",
    "            eta_grads = neg_grads[:, :, 0]\n",
    "            k_grads = neg_grads[:, :, 1]\n",
    "            w_grads = neg_grads[:, :, 2]\n",
    "\n",
    "            for i in range(self.heads):\n",
    "                self.eta_heads[i].append(self._fit_base_learner(X_train, eta_grads[:, i]))\n",
    "                self.k_heads[i].append(self._fit_base_learner(X_train, k_grads[:, i]))\n",
    "                if not self.uniform_heads:\n",
    "                    self.w_heads[i].append(self._fit_base_learner(X_train, w_grads[:, i]))\n",
    "\n",
    "            if self.patience is not None:\n",
    "                survs = self.predict(X_val, timeline)\n",
    "                mean_times = survs.sum(axis=1)\n",
    "                cid = concordance_index_td(y_val[\"event\"], y_val[\"time\"], -mean_times)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Iteration {j:04}] Concordance: {cid:.4f}\")\n",
    "                if cid > best_cid:\n",
    "                    best_cid = cid\n",
    "                    best_num_base_learners = len(self.eta_heads[0])\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        break\n",
    "\n",
    "        if self.patience is not None:\n",
    "            self.eta_heads = [heads[:best_num_base_learners] for heads in self.eta_heads]\n",
    "            self.k_heads = [heads[:best_num_base_learners] for heads in self.k_heads]\n",
    "            self.w_heads = [heads[:best_num_base_learners] for heads in self.w_heads]\n",
    "\n",
    "    def predict(self, X_test, times) -> np.array:\n",
    "        times = times.reshape((1, 1, -1))\n",
    "        params = torch.tensor(self._predict_params(X_test)).float()\n",
    "\n",
    "        etas = F.relu(params[:, :, 0]).numpy().reshape((-1, self.heads, 1))\n",
    "        ks = F.relu(params[:, :, 1]).numpy().reshape((-1, self.heads, 1))\n",
    "        ws = self.heads_activation_fn(params[:, :, 2]).numpy().reshape((-1, self.heads, 1))\n",
    "\n",
    "        cum_hazard = np.zeros((len(X_test), len(times[0][0])))\n",
    "\n",
    "        if self.weibull_heads > 0:\n",
    "            weibull_cum_hazard = self._weibull_cum_hazard(\n",
    "                etas[:, : self.weibull_heads], ks[:, : self.weibull_heads], times\n",
    "            )\n",
    "            cum_hazard += (weibull_cum_hazard * ws[:, : self.weibull_heads]).sum(axis=1)\n",
    "\n",
    "        if self.loglogistic_heads > 0:\n",
    "            loglogistic_cum_hazard = self._loglogistic_cum_hazard(\n",
    "                etas[:, self.weibull_heads :], ks[:, self.weibull_heads :], times\n",
    "            )\n",
    "            cum_hazard += (loglogistic_cum_hazard * ws[:, self.weibull_heads :]).sum(axis=1)\n",
    "        surv = np.exp(-cum_hazard)\n",
    "        return surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model: str, params: dict) -> SurvModel:\n",
    "    \"\"\"Initializes a survival model with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        model: String with the model name.\n",
    "        params: Dictionary with the model parameters.\n",
    "\n",
    "    Returns:\n",
    "        Initialized survival model.\n",
    "    \"\"\"\n",
    "\n",
    "    models = {\n",
    "        \"cox\": Cox,\n",
    "        \"rsf\": RandomSurvivalForest,\n",
    "        \"deepsurv\": DeepSurv,\n",
    "        \"deephit\": DeepHit,\n",
    "        \"dsm\": DeepSurvivalMachines,\n",
    "        \"coxboost\": CoxBoost,\n",
    "        \"fpboost\": FPBoost,\n",
    "    }\n",
    "    return models[model](**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_training(dataset: str, model: str, params: dict) -> dict[str, float]:\n",
    "    \"\"\"Trains and evaluates a model on a dataset.\"\"\"\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    df = load_dataframe(dataset)\n",
    "\n",
    "    metrics = {}\n",
    "    for train, val, test in get_k_fold_splits(df):\n",
    "        X_train, y_train = train\n",
    "        X_val, y_val = val\n",
    "        X_test, y_test = test\n",
    "\n",
    "        m = init_model(model, params)\n",
    "        m.fit(X_train, y_train)\n",
    "        val_results = m.evaluate(X_val, y_val, y_train)\n",
    "        test_results = m.evaluate(X_test, y_test, y_train)\n",
    "\n",
    "        for k, v in val_results.items():\n",
    "            k = f\"{k}_val\"\n",
    "            if k not in metrics:\n",
    "                metrics[k] = []\n",
    "            metrics[k].append(v)\n",
    "        for k, v in test_results.items():\n",
    "            k = f\"{k}_test\"\n",
    "            if k not in metrics:\n",
    "                metrics[k] = []\n",
    "            metrics[k].append(v)\n",
    "\n",
    "    ret = {}\n",
    "    for k, v in metrics.items():\n",
    "        ret[f\"{k}_mean\"] = np.mean(v).item()\n",
    "        ret[f\"{k}_std\"] = np.std(v).item()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Train and evaluate the baseline models and FPBoost on the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CPUS = 8  # Number of CPUs for parallel training\n",
    "OBJ_MEMORY_GB = 2  # Memory for each ray object in GB\n",
    "\n",
    "RESULTS_PATH = \"results\"  # Path to save the results\n",
    "\n",
    "# Datasets on which to train the models\n",
    "DATASETS = [\n",
    "    \"aids\",\n",
    "    \"breast_cancer\",\n",
    "    \"gbsg\",\n",
    "    \"metabric\",\n",
    "    \"support\",\n",
    "    \"veterans\",\n",
    "    \"whas\",\n",
    "]\n",
    "\n",
    "# Baseline models to train\n",
    "BASELINE_MODELS = [\"rsf\", \"cox\", \"coxboost\", \"deepsurv\", \"dsm\", \"deephit\"]\n",
    "\n",
    "# Search space for the hyperparameter optimization of the FPBoost model\n",
    "SEARCH_SPACE = {\n",
    "    \"weibull_heads\": tune.randint(0, 8),\n",
    "    \"loglogistic_heads\": tune.randint(0, 8),\n",
    "    \"n_estimators\": tune.randint(1, 256),\n",
    "    \"max_depth\": tune.randint(1, 8),\n",
    "    \"learning_rate\": tune.uniform(1e-2, 1),\n",
    "    \"alpha\": tune.uniform(0.0, 1.0),\n",
    "    \"l1_ratio\": tune.uniform(0, 1),\n",
    "    \"uniform_heads\": tune.choice([True, False]),\n",
    "    \"heads_activation\": tune.choice([\"relu\", \"softmax\"]),\n",
    "    \"patience\": tune.choice([None, 4, 16]),\n",
    "}\n",
    "\n",
    "ITERATIONS = 8  # Number of iterations for the hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "ray.init(\n",
    "    num_cpus=NUM_CPUS,\n",
    "    object_store_memory=OBJ_MEMORY_GB * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = os.path.join(RESULTS_PATH, \"temp_baselines\")\n",
    "os.makedirs(tempdir, exist_ok=True)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def remote_baseline_training(dataset: str, model: str) -> dict[str, float]:\n",
    "    if os.path.exists(os.path.join(tempdir, f\"{dataset}_{model}.json\")):\n",
    "        return None\n",
    "    ret = atomic_training(dataset, model, {})\n",
    "    ret[\"dataset\"] = dataset\n",
    "    ret[\"model\"] = model\n",
    "    ret[\"params\"] = {}\n",
    "    with open(os.path.join(tempdir, f\"{dataset}_{model}.json\"), \"w\") as f:\n",
    "        json.dump(ret, f)\n",
    "    return ret\n",
    "\n",
    "\n",
    "BASELINE_RESULTS_FILE = os.path.join(RESULTS_PATH, \"baseline_results.csv\")\n",
    "\n",
    "baseline_results = []\n",
    "for dataset in DATASETS:\n",
    "    for model in BASELINE_MODELS:\n",
    "        baseline_results.append(remote_baseline_training.remote(dataset, model))\n",
    "baseline_results = ray.get(baseline_results)\n",
    "\n",
    "results = {}\n",
    "for f in os.listdir(tempdir):\n",
    "    with open(os.path.join(tempdir, f), \"r\") as file:\n",
    "        r = json.load(file)\n",
    "        for k, v in r.items():\n",
    "            if k not in results:\n",
    "                results[k] = []\n",
    "            results[k].append(v)\n",
    "\n",
    "baseline_results_df = pd.DataFrame(results)\n",
    "baseline_results_df.to_csv(BASELINE_RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    return atomic_training(config[\"dataset\"], \"fpboost\", config[\"params\"])\n",
    "\n",
    "\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(f\"Training FPBoost on {dataset}...\")\n",
    "\n",
    "    search_alg = OptunaSearch(\n",
    "        metric=\"cid_val_mean\", mode=\"max\", sampler=optuna.samplers.TPESampler()\n",
    "    )\n",
    "\n",
    "    analysis = tune.run(\n",
    "        objective,\n",
    "        config={\"dataset\": dataset, \"params\": SEARCH_SPACE},\n",
    "        num_samples=ITERATIONS,\n",
    "        search_alg=search_alg,\n",
    "        name=f\"{dataset}_{model}_tune_optuna_experiment\",\n",
    "        storage_path=f\"file://{os.path.abspath(RESULTS_PATH)}\",\n",
    "    )\n",
    "\n",
    "    df = analysis.results_df\n",
    "    csv_filename = f\"{dataset}_{model}_tune_optuna_experiment.csv\"\n",
    "    df.to_csv(os.path.join(RESULTS_PATH, csv_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpboost_results = []\n",
    "for file in os.listdir(RESULTS_PATH):\n",
    "    if file.endswith(\"_tune_optuna_experiment.csv\"):\n",
    "        df = pd.read_csv(os.path.join(RESULTS_PATH, file))\n",
    "        fpboost_results.append(df)\n",
    "\n",
    "df = pd.concat(fpboost_results)\n",
    "df[\"model\"] = \"fpboost\"\n",
    "df = df.dropna(axis=1, how=\"any\")\n",
    "df.columns = df.columns.str.replace(\"config/\", \"\")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select the best hyperparameters for each dataset accrding to the C-Index - IBS difference on the validation set\n",
    "df[\"sel_col\"] = df[\"cid_val_mean\"] - df[\"ibs_val_mean\"]\n",
    "idx = df.groupby([\"dataset\", \"model\"])[\"sel_col\"].idxmax()\n",
    "df = df.loc[idx]\n",
    "assert df.groupby([\"dataset\", \"model\"]).size().eq(1).all()\n",
    "\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"sel_col\",\n",
    "        \"timestamp\",\n",
    "        \"time_since_restore\",\n",
    "        \"pid\",\n",
    "        \"time_total_s\",\n",
    "        \"date\",\n",
    "        \"training_iteration\",\n",
    "        \"time_this_iter_s\",\n",
    "        \"done\",\n",
    "        \"hostname\",\n",
    "        \"node_ip\",\n",
    "        \"iterations_since_restore\",\n",
    "        \"experiment_tag\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "FPBOOST_RESULTS_FILE = os.path.join(RESULTS_PATH, \"fpboost_results.csv\")\n",
    "\n",
    "df.to_csv(FPBOOST_RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Collection\n",
    "\n",
    "Load the results returned by the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_df = pd.read_csv(BASELINE_RESULTS_FILE)\n",
    "fpboost_results_df = pd.read_csv(FPBOOST_RESULTS_FILE)\n",
    "\n",
    "results_df = pd.concat([baseline_results_df, fpboost_results_df])\n",
    "results_df.sort_values([\"dataset\", \"model\"], inplace=True)\n",
    "results_df.to_csv(os.path.join(RESULTS_PATH, \"results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
